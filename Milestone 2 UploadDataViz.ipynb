{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UploadDataViz.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8Dmidsc_Ijb"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.colab import drive\n",
        "from google.oauth2 import service_account\n",
        "import json, os\n",
        "import pandas as pd \n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Cloud services\n",
        "gcp_service_account_credentials_json_filename = 'epfl-course-f41b0ed796f9.json' #need to upload the json credential files to the root directory of the google colab files\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_service_account_credentials_json_filename\n",
        "credentials = service_account.Credentials.from_service_account_file(gcp_service_account_credentials_json_filename, scopes=['https://www.googleapis.com/auth/bigquery', 'https://www.googleapis.com/auth/drive'])\n",
        "project_id = 'epfl-course'\n",
        "bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "bigquery_client = bigquery.Client()"
      ],
      "metadata": {
        "id": "O7aCiVDo_cRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import sample data\n",
        "iris = sns.load_dataset('iris')"
      ],
      "metadata": {
        "id": "fMIPELUYCB-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try to upload to bq\n",
        "table_id = \"epfl-course.datavizcourse.sample_data\"\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "\n",
        "job = bigquery_client.load_table_from_dataframe(\n",
        "    iris, table_id, job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete.\n"
      ],
      "metadata": {
        "id": "NhnHjjETCm2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = bigquery_client.get_table(table_id)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), table_id\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "RBMP24WlEE5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the content from google drive\n",
        "path = r\"/content/drive/MyDrive/dataviz/data/\"\n",
        "drive.mount('/content/drive')\n",
        "file_list=os.listdir(path)"
      ],
      "metadata": {
        "id": "L8chWWqlEMMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_files = []\n",
        "for f in file_list:\n",
        "  if 'event' in f:\n",
        "    event_files.append(f)\n",
        "\n",
        "print('We have ' + str(len(event_files)) + ' event files')"
      ],
      "metadata": {
        "id": "S0-n16-EFjLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the sizes:\n",
        "sizes = {}\n",
        "for e in event_files:\n",
        "  sizes[e] = os.path.getsize(path + e)"
      ],
      "metadata": {
        "id": "HsuJ7h00FyAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to simulate the behavior without importing"
      ],
      "metadata": {
        "id": "Wu-0wM8NTkuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_names = [x[0] for x in sorted(sizes.items(), key=lambda item: int(item[1]))][3:]\n",
        "sorted_sizes = [x[1] for x in sorted(sizes.items(), key=lambda item: int(item[1]))][3:]\n",
        "sorted_sizes_giga = [ x / 10**9 for x in sorted_sizes]\n",
        "sum(sorted_sizes_giga)"
      ],
      "metadata": {
        "id": "XMTnuhhDGKa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read a table\n",
        "while len(sorted_names) > 1:\n",
        "  big_one = pd.read_csv(path+event_files[0])\n",
        "  sorted_names.pop(0)\n",
        "  sorted_sizes.pop(0)\n",
        "  sorted_sizes_giga.pop(0)\n",
        "\n",
        "  #create a 1 gb table\n",
        "  giga_count = 0\n",
        "  for name, size, size_giga in zip(sorted_names, sorted_sizes, sorted_sizes_giga):\n",
        "    #check the total lenght with following element\n",
        "    giga_count += size_giga\n",
        "    if giga_count < 0.35:\n",
        "      #include following element\n",
        "     # new_df = pd.read_csv(path+name)\n",
        "      #big_one = pd.concat([big_one, new_df])\n",
        "\n",
        "      #delete element from to add list\n",
        "      sorted_names.pop(0)\n",
        "      sorted_sizes.pop(0)\n",
        "      sorted_sizes_giga.pop(0)\n",
        "      print('left:', len(sorted_names), 'giga', giga_count)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  #add the table to bq\n",
        "  print('#############################################')\n",
        "  print('fai qualcosa con bq')"
      ],
      "metadata": {
        "id": "G2OOcUIqTESa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real deal"
      ],
      "metadata": {
        "id": "BwXB8HlTThqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_names = [x[0] for x in sorted(sizes.items(), key=lambda item: int(item[1]))][3:]\n",
        "sorted_sizes = [x[1] for x in sorted(sizes.items(), key=lambda item: int(item[1]))][3:]\n",
        "sorted_sizes_giga = [ x / 10**9 for x in sorted_sizes]\n",
        "sum(sorted_sizes_giga)"
      ],
      "metadata": {
        "id": "rokTrgH3T2ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_one = pd.read_csv(path+event_files[10])\n",
        "big_one.head()"
      ],
      "metadata": {
        "id": "NwnZOEGGKBLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# big_one['individual_local_identifier'] = big_one.apply(lambda x: str(x['individual_local_identifier']) , axis = 1)"
      ],
      "metadata": {
        "id": "Y7LBju_PMt3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# big_one['individual_local_identifier'] = big_one['individual_local_identifier'].astype(str)\n",
        "# big_one['individual_taxon_canonical_name'] = big_one['individual_taxon_canonical_name'].astype(str)"
      ],
      "metadata": {
        "id": "dzgT1iiCOFXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# table_id = \"epfl-course.datavizcourse.temp\"\n",
        "# job_config = bigquery.LoadJobConfig()\n",
        "\n",
        "# job = bigquery_client.load_table_from_dataframe(\n",
        "    # big_one, table_id, job_config=job_config\n",
        "# )  # Make an API request.\n",
        "# job.result()  # Wait for the job to complete."
      ],
      "metadata": {
        "id": "vcHfxPwAOs5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unprocessed = []\n",
        "#read a table\n",
        "while len(sorted_names) > 1:\n",
        "  big_one = pd.read_csv(path+event_files[0])\n",
        "  giga_count = sorted_sizes_giga[0]\n",
        "\n",
        "  sorted_names.pop(0)\n",
        "  sorted_sizes.pop(0)\n",
        "  sorted_sizes_giga.pop(0)\n",
        "\n",
        "  #create a 1 gb table\n",
        "  giga_count = 0\n",
        "  temp = []\n",
        "  for name, size, size_giga in zip(sorted_names, sorted_sizes, sorted_sizes_giga):\n",
        "    #check the total lenght with following element\n",
        "    giga_count += size_giga\n",
        "    if giga_count < 0.10:\n",
        "      #include following element\n",
        "      new_df = pd.read_csv(path+name)\n",
        "      big_one = pd.concat([big_one, new_df])\n",
        "\n",
        "      #delete element from to add list\n",
        "      sorted_names.pop(0)\n",
        "      sorted_sizes.pop(0)\n",
        "      sorted_sizes_giga.pop(0)\n",
        "      print('left:', len(sorted_names), 'giga', giga_count)\n",
        "\n",
        "      temp += [name]\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  try:\n",
        "    big_one['individual_local_identifier'] = big_one['individual_local_identifier'].astype(str)\n",
        "    big_one['individual_taxon_canonical_name'] = big_one['individual_taxon_canonical_name'].astype(str)\n",
        "\n",
        "    #add the table to bq\n",
        "    table_id = \"epfl-course.datavizcourse.tot_events_2\"\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "\n",
        "    job = bigquery_client.load_table_from_dataframe(\n",
        "        big_one, table_id, job_config=job_config\n",
        "    )  # Make an API request.\n",
        "    job.result()  # Wait for the job to complete.\n",
        "  except Exception as e: \n",
        "    print(e)"
      ],
      "metadata": {
        "id": "FkQorZakTgNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to delete \n",
        "#read a table\n",
        "while len(sorted_names) > 1:\n",
        "  big_one = pd.read_csv(path+event_files[0])\n",
        "  sorted_names.pop(0)\n",
        "  sorted_sizes.pop(0)\n",
        "  sorted_sizes_giga.pop(0)\n",
        "\n",
        "  #create a 1 gb table\n",
        "  giga_count = 0\n",
        "  for name, size, size_giga in zip(sorted_names, sorted_sizes, sorted_sizes_giga):\n",
        "    #check the total lenght with following element\n",
        "    giga_count += size_giga\n",
        "    if giga_count < 0.35:\n",
        "      #include following element\n",
        "      new_df = pd.read_csv(path+name)\n",
        "      big_one = pd.concat([big_one, new_df])\n",
        "\n",
        "      #delete element from to add list\n",
        "      sorted_names.pop(0)\n",
        "      sorted_sizes.pop(0)\n",
        "      sorted_sizes_giga.pop(0)\n",
        "      print('left:', len(sorted_names), 'giga', giga_count)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  #add the table to bq\n",
        "  table_id = \"epfl-course.ada_project.tot_events\"\n",
        "  job_config = bigquery.LoadJobConfig()\n",
        "\n",
        "  job = bigquery_client.load_table_from_dataframe(\n",
        "      iris, table_id, job_config=job_config\n",
        "  )  # Make an API request.\n",
        "  job.result()  # Wait for the job to complete."
      ],
      "metadata": {
        "id": "oh8HtQZmWB28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1k54Q8wLTrwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read a table\n",
        "while len(sorted_names) > 1:\n",
        "  big_one = pd.read_csv(path+event_files[0])\n",
        "  sorted_names.pop(0)\n",
        "  sorted_sizes.pop(0)\n",
        "  sorted_sizes_giga.pop(0)\n",
        "\n",
        "  #create a 1 gb table\n",
        "  giga_count = 0\n",
        "  for name, size, size_giga in zip(sorted_names, sorted_sizes, sorted_sizes_giga):\n",
        "    #check the total lenght with following element\n",
        "    giga_count += size_giga\n",
        "    if giga_count < 0.35:\n",
        "      #include following element\n",
        "      new_df = pd.read_csv(path+name)\n",
        "      big_one = pd.concat([big_one, new_df])\n",
        "\n",
        "      #delete element from to add list\n",
        "      sorted_names.pop(0)\n",
        "      sorted_sizes.pop(0)\n",
        "      sorted_sizes_giga.pop(0)\n",
        "      print('left:', len(sorted_names), 'giga', giga_count)\n",
        "\n",
        "  #add the table to bq\n",
        "  table_id = \"epfl-course.ada_project.tot_events\"\n",
        "  job_config = bigquery.LoadJobConfig()\n",
        "\n",
        "  job = bigquery_client.load_table_from_dataframe(\n",
        "      iris, table_id, job_config=job_config\n",
        "  )  # Make an API request.\n",
        "  job.result()  # Wait for the job to complete."
      ],
      "metadata": {
        "id": "bZyS9UvSJujf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_one = pd.read_csv(path+'studies.csv.gz')"
      ],
      "metadata": {
        "id": "p6s2zipmZbGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_one.head()"
      ],
      "metadata": {
        "id": "ZnyiKQ-jZgV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add the table to bq\n",
        "table_id = \"epfl-course.datavizcourse.studies_names\"\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "\n",
        "job = bigquery_client.load_table_from_dataframe(\n",
        "    big_one, table_id, job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete."
      ],
      "metadata": {
        "id": "UvFmQuo9Zx2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}